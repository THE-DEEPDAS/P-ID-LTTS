{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6ba56c0",
   "metadata": {},
   "source": [
    "# PID TikZ Pipeline (Local GPU-Friendly Notebook)\n",
    "This notebook mirrors the end-to-end pipeline in a laptop-friendly, restartable format. It keeps everything self-contained by downloading models into the working directory and shielding the run from GPU out-of-memory events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d36d9ce",
   "metadata": {},
   "source": [
    "## 1. Load Configuration and Working Paths\n",
    "Define reusable parameters, ensure required packages are present, and prepare the local directory structure where artifacts and models will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c68f6069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Python 3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\anaconda3\\envs\\ai-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root        : d:\\P&ID LTTS\n",
      "Artifacts directory : d:\\P&ID LTTS\\notebook_artifacts\n",
      "Models directory    : d:\\P&ID LTTS\\local_models\n",
      "TikZ source path    : D:\\P&ID LTTS\\test\\main.tex\n"
     ]
    }
   ],
   "source": [
    "# Section 1: imports, package checks, and configuration\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Using Python {sys.version}\")\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    \"torch\",\n",
    "    \"transformers>=4.44.0\",\n",
    "    \"accelerate>=0.33.0\",\n",
    "    \"bitsandbytes>=0.43.1\",\n",
    "    \"huggingface_hub>=0.23.0\",\n",
    "]\n",
    "\n",
    "def ensure_package(spec: str) -> None:\n",
    "    package_key = spec.split(\"==\")[0].split(\">=\")[0].replace(\"-\", \"_\")\n",
    "    try:\n",
    "        __import__(package_key)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {spec} ...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", spec])\n",
    "\n",
    "for requirement in REQUIRED_PACKAGES:\n",
    "    ensure_package(requirement)\n",
    "\n",
    "from datetime import datetime  # noqa: E402 (import after potential installs)\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / \"notebook_artifacts\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"local_models\"\n",
    "TIKZ_INPUT_DIR = PROJECT_ROOT / \"tikz_inputs\"\n",
    "for directory in (ARTIFACTS_DIR, MODELS_DIR, TIKZ_INPUT_DIR):\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONFIG = {\n",
    "    \"tikz_path\": \"D:\\\\P&ID LTTS\\\\test\\\\main.tex\",\n",
    "    \"tikz_to_text_model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"text_to_tikz_model\": \"nllg/detikzify-v2.5-8b\",\n",
    "    \"hf_token\": os.environ.get(\"HF_TOKEN\"),\n",
    "    \"compile_pdf\": False,\n",
    "    \"job_name\": f\"pid_notebook_run_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\",\n",
    "    \"max_tokens_description\": 1024,\n",
    "    \"temperature_description\": 0.3,\n",
    "    \"top_p_description\": 0.9,\n",
    "    \"repetition_penalty_description\": 1.05,\n",
    "    \"max_tokens_regen\": 768,\n",
    "    \"temperature_regen\": 0.2,\n",
    "    \"top_p_regen\": 0.95,\n",
    "    \"repetition_penalty_regen\": 1.0,\n",
    "}\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)8s | %(message)s\")\n",
    "\n",
    "RUN_STATE: dict = {}\n",
    "MODEL_CACHE: dict = {}\n",
    "\n",
    "print(f\"Project root        : {PROJECT_ROOT}\")\n",
    "print(f\"Artifacts directory : {ARTIFACTS_DIR}\")\n",
    "print(f\"Models directory    : {MODELS_DIR}\")\n",
    "print(f\"TikZ source path    : {CONFIG['tikz_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e8f0bc",
   "metadata": {},
   "source": [
    "## 2. Ensure GPU Availability with Memory-Safe Fallback\n",
    "Inspect CUDA availability, report memory capacity, and register helpers that deallocate GPU memory so the notebook can continue after an out-of-memory exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64421235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device detected: Quadro M1000M with 4.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Section 2: device inspection and OOM helpers\n",
    "import torch\n",
    "\n",
    "DEVICE_INFO = {\n",
    "    \"device\": torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"),\n",
    "    \"memory_gb\": None,\n",
    "}\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    DEVICE_INFO[\"memory_gb\"] = round(props.total_memory / (1024 ** 3), 2)\n",
    "    print(f\"CUDA device detected: {props.name} with {DEVICE_INFO['memory_gb']} GB\")\n",
    "else:\n",
    "    print(\"CUDA device not detected; computations will fall back to CPU.\")\n",
    "\n",
    "def recover_from_oom(stage: str, error: BaseException) -> None:\n",
    "    \"\"\"Release GPU resources and log the out-of-memory event.\"\"\"\n",
    "    logging.warning(\"[OOM] %s encountered: %s\", stage, error)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    print(f\"Recovered from OOM in {stage}; continuing on CPU where necessary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8e25d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tikz_to_text] Using cached weights at d:\\P&ID LTTS\\local_models\\meta-llama_Llama-3.1-8B-Instruct\n",
      "[text_to_tikz] Using cached weights at d:\\P&ID LTTS\\local_models\\nllg_detikzify-v2.5-8b\n",
      "Model cache entries:\n",
      "  tikz_to_text: d:\\P&ID LTTS\\local_models\\meta-llama_Llama-3.1-8B-Instruct\n",
      "  text_to_tikz: d:\\P&ID LTTS\\local_models\\nllg_detikzify-v2.5-8b\n",
      "[text_to_tikz] Using cached weights at d:\\P&ID LTTS\\local_models\\nllg_detikzify-v2.5-8b\n",
      "Model cache entries:\n",
      "  tikz_to_text: d:\\P&ID LTTS\\local_models\\meta-llama_Llama-3.1-8B-Instruct\n",
      "  text_to_tikz: d:\\P&ID LTTS\\local_models\\nllg_detikzify-v2.5-8b\n"
     ]
    }
   ],
   "source": [
    "# Section 3: model download utilities\n",
    "from typing import Optional\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import socket\n",
    "\n",
    "from huggingface_hub import snapshot_download, hf_hub_download\n",
    "\n",
    "try:  # Newer huggingface_hub versions expose HfHubHTTPError at top level\n",
    "    from huggingface_hub import HfHubHTTPError  # type: ignore[attr-defined]\n",
    "except Exception:\n",
    "    try:  # Older releases keep it under huggingface_hub.utils\n",
    "        from huggingface_hub.utils import HfHubHTTPError  # type: ignore\n",
    "    except Exception:  # Fallback when the helper is absent altogether\n",
    "        class HfHubHTTPError(Exception):\n",
    "            \"\"\"Fallback exception used when huggingface_hub lacks HfHubHTTPError.\"\"\"\n",
    "\n",
    "try:\n",
    "    from huggingface_hub.errors import LocalEntryNotFoundError  # type: ignore[attr-defined]\n",
    "except Exception:\n",
    "    try:  # Some releases expose it under huggingface_hub.utils\n",
    "        from huggingface_hub.utils import LocalEntryNotFoundError  # type: ignore\n",
    "    except Exception:\n",
    "        class LocalEntryNotFoundError(Exception):\n",
    "            \"\"\"Fallback raised when huggingface_hub omits LocalEntryNotFoundError.\"\"\"\n",
    "\n",
    "try:\n",
    "    from huggingface_hub.utils import RepositoryNotFoundError  # type: ignore\n",
    "except Exception:  # pragma: no cover - older releases reuse HfHubHTTPError\n",
    "    RepositoryNotFoundError = HfHubHTTPError  # type: ignore\n",
    "\n",
    "try:\n",
    "    from requests.exceptions import ConnectionError as RequestsConnectionError, ConnectTimeout\n",
    "except Exception:  # pragma: no cover - requests should be present, but guard anyway\n",
    "    RequestsConnectionError = ConnectTimeout = type(\"RequestsConnectionError\", (Exception,), {})\n",
    "\n",
    "\n",
    "def _safe_repo_dirname(repo_id: str) -> str:\n",
    "    # Make a filesystem-safe name for the repo identifier\n",
    "    return re.sub(r\"[^0-9A-Za-z._-]\", \"_\", repo_id)\n",
    "\n",
    "\n",
    "def _clean_incomplete_files(target_dir: Path, cache_key: str) -> None:\n",
    "    \"\"\"Remove any partially downloaded blobs so retries start fresh.\"\"\"\n",
    "    partials = list(target_dir.rglob(\"*.incomplete\"))\n",
    "    if not partials:\n",
    "        return\n",
    "    for partial in partials:\n",
    "        try:\n",
    "            partial.unlink()\n",
    "        except OSError as unlink_err:\n",
    "            logging.warning(\"[%s] Could not remove partial file %s: %s\", cache_key, partial, unlink_err)\n",
    "\n",
    "\n",
    "def _download_missing_shards(\n",
    "    repo_id: str,\n",
    "    cache_key: str,\n",
    "    target_dir: Path,\n",
    "    token: Optional[str],\n",
    "    *,\n",
    "    max_retry: int,\n",
    ") -> None:\n",
    "    \"\"\"Ensure sharded safetensors referenced by the index are present.\"\"\"\n",
    "    index_path = target_dir / \"model.safetensors.index.json\"\n",
    "    if not index_path.exists():\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        index_data = json.loads(index_path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as err:\n",
    "        logging.warning(\"[%s] Unable to parse safetensor index for shard reconciliation: %s\", cache_key, err)\n",
    "        return\n",
    "\n",
    "    weight_map = index_data.get(\"weight_map\", {})\n",
    "    if not weight_map:\n",
    "        return\n",
    "\n",
    "    required_files = {Path(filename).name for filename in weight_map.values()}\n",
    "    missing_files = [name for name in required_files if not (target_dir / name).exists()]\n",
    "    if not missing_files:\n",
    "        return\n",
    "\n",
    "    logging.info(\"[%s] Detected %s missing shard(s); downloading individually.\", cache_key, len(missing_files))\n",
    "    for filename in missing_files:\n",
    "        for attempt in range(1, max_retry + 1):\n",
    "            try:\n",
    "                hf_hub_download(\n",
    "                    repo_id=repo_id,\n",
    "                    filename=filename,\n",
    "                    repo_type=\"model\",\n",
    "                    local_dir=target_dir.as_posix(),\n",
    "                    token=token,\n",
    "                )\n",
    "                break\n",
    "            except HfHubHTTPError as err:\n",
    "                status_code = getattr(getattr(err, \"response\", None), \"status_code\", None)\n",
    "                if status_code == 401:\n",
    "                    raise RuntimeError(\n",
    "                        f\"Unauthorized when fetching shard '{filename}' of '{repo_id}'. Provide a valid HF_TOKEN with access to the repository.\"\n",
    "                    ) from err\n",
    "                raise\n",
    "            except LocalEntryNotFoundError as err:\n",
    "                logging.warning(\n",
    "                    \"[%s] Shard %s attempt %s/%s failed due to transient hub lookup error; retrying soon...\",\n",
    "                    cache_key,\n",
    "                    filename,\n",
    "                    attempt,\n",
    "                    max_retry,\n",
    "                )\n",
    "                if attempt == max_retry:\n",
    "                    raise\n",
    "                time.sleep(min(30, 5 * attempt))\n",
    "            except (RequestsConnectionError, ConnectTimeout, socket.gaierror) as err:\n",
    "                logging.warning(\n",
    "                    \"[%s] Shard %s attempt %s/%s failed due to network resolution error (%s); retrying soon...\",\n",
    "                    cache_key,\n",
    "                    filename,\n",
    "                    attempt,\n",
    "                    max_retry,\n",
    "                    err,\n",
    "                )\n",
    "                if attempt == max_retry:\n",
    "                    raise RuntimeError(\n",
    "                        f\"Network failure while downloading shard '{filename}' from '{repo_id}'. Please verify connectivity and retry.\"\n",
    "                    ) from err\n",
    "                time.sleep(min(30, 5 * attempt))\n",
    "            except RuntimeError as err:\n",
    "                if \"CAS service error\" in str(err):\n",
    "                    logging.warning(\n",
    "                        \"[%s] Shard %s attempt %s/%s hit CAS service error; retrying soon...\",\n",
    "                        cache_key,\n",
    "                        filename,\n",
    "                        attempt,\n",
    "                        max_retry,\n",
    "                    )\n",
    "                    if attempt == max_retry:\n",
    "                        raise\n",
    "                    time.sleep(min(30, 5 * attempt))\n",
    "                else:\n",
    "                    raise\n",
    "        else:  # pragma: no cover - defensive safeguard\n",
    "            raise RuntimeError(f\"Failed to download shard '{filename}' after {max_retry} retries.\")\n",
    "\n",
    "\n",
    "def ensure_model_local(repo_id: str, cache_key: str, token: Optional[str] = None, *, max_retry: int = 3) -> Path:\n",
    "    target_dir = MODELS_DIR / _safe_repo_dirname(repo_id)\n",
    "    marker_file = target_dir / \".completed\"\n",
    "    if marker_file.exists() and target_dir.exists():\n",
    "        print(f\"[{cache_key}] Using cached weights at {target_dir}\")\n",
    "        MODEL_CACHE[cache_key] = target_dir\n",
    "        return target_dir\n",
    "\n",
    "    print(f\"[{cache_key}] Downloading snapshot {repo_id} into {target_dir}\")\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for attempt in range(1, max_retry + 1):\n",
    "        _clean_incomplete_files(target_dir, cache_key)\n",
    "        try:\n",
    "            snapshot_download(\n",
    "                repo_id=repo_id,\n",
    "                repo_type=\"model\",\n",
    "                local_dir=target_dir.as_posix(),\n",
    "                token=token,\n",
    "            )\n",
    "            _download_missing_shards(\n",
    "                repo_id,\n",
    "                cache_key,\n",
    "                target_dir,\n",
    "                token,\n",
    "                max_retry=max_retry,\n",
    "            )\n",
    "            marker_file.touch()\n",
    "            MODEL_CACHE[cache_key] = target_dir\n",
    "            print(f\"[{cache_key}] Download complete into {target_dir}\")\n",
    "            return target_dir\n",
    "        except RepositoryNotFoundError as err:\n",
    "            raise RuntimeError(\n",
    "                f\"Model '{repo_id}' not found or access denied. Accept the repository license and ensure HF_TOKEN is set if required.\"\n",
    "            ) from err\n",
    "        except HfHubHTTPError as err:\n",
    "            if getattr(err, \"response\", None) is not None and getattr(err.response, \"status_code\", None) == 401:\n",
    "                raise RuntimeError(\n",
    "                    f\"Unauthorized when fetching '{repo_id}'. Provide a valid HF_TOKEN with access to the repository.\"\n",
    "                ) from err\n",
    "            raise\n",
    "        except LocalEntryNotFoundError as err:\n",
    "            logging.warning(\n",
    "                \"[%s] Snapshot attempt %s/%s failed due to transient hub lookup error; retrying soon...\",\n",
    "                cache_key,\n",
    "                attempt,\n",
    "                max_retry,\n",
    "            )\n",
    "            if attempt == max_retry:\n",
    "                logging.error(\"[%s] Exhausted retries due to repeated hub lookup failures.\", cache_key)\n",
    "                raise\n",
    "            time.sleep(min(30, 5 * attempt))\n",
    "        except (RequestsConnectionError, ConnectTimeout, socket.gaierror) as err:\n",
    "            logging.warning(\n",
    "                \"[%s] Snapshot attempt %s/%s failed due to network resolution error (%s); retrying soon...\",\n",
    "                cache_key,\n",
    "                attempt,\n",
    "                max_retry,\n",
    "                err,\n",
    "            )\n",
    "            if attempt == max_retry:\n",
    "                logging.error(\"[%s] Exhausted retries due to persistent network resolution errors.\", cache_key)\n",
    "                raise RuntimeError(\n",
    "                    f\"Network failure while downloading '{repo_id}'. Please verify internet connectivity and retry.\"\n",
    "                ) from err\n",
    "            time.sleep(min(30, 5 * attempt))\n",
    "        except RuntimeError as err:\n",
    "            if \"CAS service error\" in str(err):\n",
    "                logging.warning(\n",
    "                    \"[%s] Snapshot attempt %s/%s failed due to CAS service error; retrying soon...\",\n",
    "                    cache_key,\n",
    "                    attempt,\n",
    "                    max_retry,\n",
    "                )\n",
    "                if attempt == max_retry:\n",
    "                    logging.error(\"[%s] Exhausted retries due to persistent CAS service error.\", cache_key)\n",
    "                    raise\n",
    "                time.sleep(min(30, 5 * attempt))\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "\n",
    "MODEL_CACHE.clear()\n",
    "\n",
    "tikz_repo = CONFIG[\"tikz_to_text_model\"]\n",
    "text2tikz_repo = CONFIG[\"text_to_tikz_model\"]\n",
    "\n",
    "try:\n",
    "    tikz_path = ensure_model_local(tikz_repo, \"tikz_to_text\", CONFIG.get(\"hf_token\"))\n",
    "except Exception as e:\n",
    "    logging.exception(\"Failed to fetch tikz->text model: %s\", e)\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    text2tikz_path = ensure_model_local(text2tikz_repo, \"text_to_tikz\", CONFIG.get(\"hf_token\"))\n",
    "except Exception as e:\n",
    "    logging.exception(\"Failed to fetch text->tikz model: %s\", e)\n",
    "    raise\n",
    "\n",
    "print(\"Model cache entries:\")\n",
    "for key, path in MODEL_CACHE.items():\n",
    "    print(f\"  {key}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70256637",
   "metadata": {},
   "source": [
    "## 4. Initialise TikZ-to-Text Model with Graceful OOM Handling\n",
    "Load the analysis model from the local cache, preferring 4-bit quantisation when available and falling back to CPU if GPU memory becomes constrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7adb8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 11:31:38,712 |     INFO | NumExpr defaulting to 8 threads.\n",
      "2025-10-21 11:31:53,700 |     INFO | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2025-10-21 11:31:53,734 |  WARNING | [TikZ->Text] Falling back to CPU load because quantised weights could not stay on GPU: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "2025-10-21 11:31:53,700 |     INFO | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2025-10-21 11:31:53,734 |  WARNING | [TikZ->Text] Falling back to CPU load because quantised weights could not stay on GPU: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:27<01:23, 27.71s/it]"
     ]
    }
   ],
   "source": [
    "# Section 4: safe model loader for TikZ -> text\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "try:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    BNB_AVAILABLE = True\n",
    "except ImportError:  # bitsandbytes not present on CPU-only machines\n",
    "    BNB_AVAILABLE = False\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GenerationDefaults:\n",
    "    max_new_tokens: int\n",
    "    temperature: float\n",
    "    top_p: float\n",
    "    repetition_penalty: float\n",
    "\n",
    "\n",
    "class SafeCausalLM:\n",
    "    def __init__(self, label: str, local_dir: Path, generation_defaults: GenerationDefaults) -> None:\n",
    "        self.label = label\n",
    "        self.local_dir = Path(local_dir)\n",
    "        self.generation_defaults = generation_defaults\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.local_dir, use_fast=True)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self._load_model(force_cpu=False)\n",
    "\n",
    "    def _quant_config(self) -> Any:\n",
    "        if self.device.type != \"cuda\" or not BNB_AVAILABLE:\n",
    "            return None\n",
    "        return BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "        )\n",
    "\n",
    "    def _load_model(self, force_cpu: bool) -> None:\n",
    "        target_device = torch.device(\"cpu\") if force_cpu or self.device.type != \"cuda\" else torch.device(\"cuda\")\n",
    "        quant_config = None if target_device.type == \"cpu\" else self._quant_config()\n",
    "        torch_dtype = torch.bfloat16 if target_device.type == \"cuda\" else torch.float32\n",
    "        try:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.local_dir,\n",
    "                device_map=\"auto\" if target_device.type == \"cuda\" else None,\n",
    "                torch_dtype=torch_dtype,\n",
    "                quantization_config=quant_config,\n",
    "            )\n",
    "            self.device = target_device\n",
    "            print(f\"[{self.label}] Loaded on {self.device} with 4-bit={quant_config is not None}\")\n",
    "        except (torch.cuda.OutOfMemoryError, RuntimeError) as err:\n",
    "            if target_device.type == \"cuda\" and \"out of memory\" in str(err).lower():\n",
    "                recover_from_oom(f\"{self.label} loading\", err)\n",
    "                self._load_model(force_cpu=True)\n",
    "            else:\n",
    "                raise\n",
    "        except ValueError as err:\n",
    "            err_str = str(err)\n",
    "            if target_device.type == \"cuda\" and (\n",
    "                \"llm_int8_enable_fp32_cpu_offload\" in err_str\n",
    "                or \"dispatched on the cpu\" in err_str.lower()\n",
    "            ):\n",
    "                logging.warning(\n",
    "                    \"[%s] Falling back to CPU load because quantised weights could not stay on GPU: %s\",\n",
    "                    self.label,\n",
    "                    err,\n",
    "                )\n",
    "                self._load_model(force_cpu=True)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    def generate(self, prompt: str, overrides: Optional[Dict[str, Any]] = None) -> str:\n",
    "        params = {\n",
    "            \"max_new_tokens\": self.generation_defaults.max_new_tokens,\n",
    "            \"temperature\": self.generation_defaults.temperature,\n",
    "            \"top_p\": self.generation_defaults.top_p,\n",
    "            \"repetition_penalty\": self.generation_defaults.repetition_penalty,\n",
    "        }\n",
    "        if overrides:\n",
    "            params.update(overrides)\n",
    "\n",
    "        attempt_cpu_fallback = False\n",
    "        while True:\n",
    "            try:\n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "                input_length = inputs[\"input_ids\"].shape[-1]\n",
    "                if self.device.type == \"cuda\":\n",
    "                    inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                with torch.inference_mode():\n",
    "                    output = self.model.generate(\n",
    "                        **inputs,\n",
    "                        do_sample=params[\"temperature\"] > 0,\n",
    "                        temperature=params[\"temperature\"],\n",
    "                        top_p=params[\"top_p\"],\n",
    "                        max_new_tokens=params[\"max_new_tokens\"],\n",
    "                        repetition_penalty=params[\"repetition_penalty\"],\n",
    "                        eos_token_id=self.tokenizer.eos_token_id,\n",
    "                        pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    )\n",
    "                generated = output[0, input_length:]\n",
    "                return self.tokenizer.decode(generated, skip_special_tokens=True).strip()\n",
    "            except (torch.cuda.OutOfMemoryError, RuntimeError) as err:\n",
    "                if \"out of memory\" in str(err).lower() and self.device.type == \"cuda\":\n",
    "                    recover_from_oom(f\"{self.label} inference\", err)\n",
    "                    self.model.to(\"cpu\")\n",
    "                    self.device = torch.device(\"cpu\")\n",
    "                    attempt_cpu_fallback = True\n",
    "                    continue\n",
    "                raise\n",
    "            finally:\n",
    "                if attempt_cpu_fallback:\n",
    "                    attempt_cpu_fallback = False\n",
    "\n",
    "\n",
    "tikz_to_text_runner = SafeCausalLM(\n",
    "    label=\"TikZ->Text\",\n",
    "    local_dir=MODEL_CACHE[\"tikz_to_text\"],\n",
    "    generation_defaults=GenerationDefaults(\n",
    "        max_new_tokens=CONFIG[\"max_tokens_description\"],\n",
    "        temperature=CONFIG[\"temperature_description\"],\n",
    "        top_p=CONFIG[\"top_p_description\"],\n",
    "        repetition_penalty=CONFIG[\"repetition_penalty_description\"],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0036633",
   "metadata": {},
   "source": [
    "## 5. Generate Natural-Language Description from TikZ Chunks\n",
    "Split the TikZ source into manageable slices, construct prompts, and run the TikZ-to-text model while absorbing transient GPU memory errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fe2ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: chunk TikZ input and produce narrative description\n",
    "import textwrap\n",
    "\n",
    "\n",
    "def chunk_text(text: str, max_chars: int = 6000, overlap: int = 500) -> list[str]:\n",
    "    if len(text) <= max_chars:\n",
    "        return [text]\n",
    "    slices: list[str] = []\n",
    "    stride = max(max_chars - overlap, 1)\n",
    "    for start in range(0, len(text), stride):\n",
    "        slices.append(text[start : start + max_chars])\n",
    "    return slices\n",
    "\n",
    "\n",
    "def build_description_prompt(tikz_snippet: str, ordinal: int) -> str:\n",
    "    return textwrap.dedent(\n",
    "        f\"\"\"\n",
    "        <s>[INST]\\nYou are a senior plant instrumentation engineer. Provide meticulous natural-language instructions to recreate a process and instrumentation diagram.\\n\\nTikZ snippet #{ordinal}:\\n```tikz\\n{tikz_snippet}\\n```\\n\\nInclude:\\n1. Numbered reconstruction steps covering all equipment, piping, instrumentation, and safety elements.\\n2. Connectivity, flow direction, and signal semantics for each tag.\\n3. Relative layout cues (e.g., left/right/up/down) to aid sketching.\\n4. A concise summary paragraph at the end.\\n[/INST]\n",
    "        \"\"\"\n",
    "    ).strip()\n",
    "\n",
    "\n",
    "tikz_path = Path(CONFIG[\"tikz_path\"])\n",
    "if not tikz_path.exists():\n",
    "    sample_tikz = textwrap.dedent(\n",
    "        r\"\"\"\n",
    "        \\begin{tikzpicture}[>=stealth]\n",
    "          \\draw[thick] (0,0) rectangle (2,2);\n",
    "          \\draw[thick,->] (2,1) -- (3.5,1) node[right]{Process flow};\n",
    "          \\node[draw,circle,minimum size=0.8cm] at (1,1) {P101};\n",
    "          \\node[draw,diamond,minimum size=0.8cm] at (0.5,1.8) {FIC-01};\n",
    "          \\draw[dashed] (0.5,1.8) -- (1,1.4);\n",
    "        \\end{tikzpicture}\n",
    "        \"\"\"\n",
    "    ).strip()\n",
    "    tikz_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tikz_path.write_text(sample_tikz, encoding=\"utf-8\")\n",
    "    print(f\"No TikZ input found. Created a sample diagram at {tikz_path} for demonstration.\")\n",
    "\n",
    "CONFIG[\"tikz_path\"] = tikz_path\n",
    "\n",
    "tikz_source = tikz_path.read_text(encoding=\"utf-8\")\n",
    "RUN_STATE[\"tikz_path\"] = str(tikz_path)\n",
    "RUN_STATE[\"tikz_source\"] = tikz_source\n",
    "\n",
    "snippets = chunk_text(tikz_source)\n",
    "print(f\"Total TikZ chunks: {len(snippets)}\")\n",
    "\n",
    "description_parts: list[str] = []\n",
    "for idx, snippet in enumerate(snippets, start=1):\n",
    "    prompt = build_description_prompt(snippet, idx)\n",
    "    try:\n",
    "        response = tikz_to_text_runner.generate(prompt)\n",
    "        description_parts.append(textwrap.dedent(response).strip())\n",
    "        print(f\"Chunk {idx} processed.\")\n",
    "    except Exception as err:  # let unexpected issues bubble up after logging\n",
    "        logging.exception(\"Description generation failed on chunk %s: %s\", idx, err)\n",
    "        raise\n",
    "\n",
    "final_description = \"\\n\\n\".join(part for part in description_parts if part)\n",
    "RUN_STATE[\"description\"] = final_description\n",
    "print(f\"Description length: {len(final_description.split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64a4a7d",
   "metadata": {},
   "source": [
    "## 6. Initialise Text-to-TikZ Model with Graceful OOM Handling\n",
    "Load the AutomaTikZ generator from the local cache using the same defensive strategy so the notebook can recover from GPU constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc4704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: load AutomaTikZ generator\n",
    "text_to_tikz_runner = SafeCausalLM(\n",
    "    label=\"Text->TikZ\",\n",
    "    local_dir=MODEL_CACHE[\"text_to_tikz\"],\n",
    "    generation_defaults=GenerationDefaults(\n",
    "        max_new_tokens=CONFIG[\"max_tokens_regen\"],\n",
    "        temperature=CONFIG[\"temperature_regen\"],\n",
    "        top_p=CONFIG[\"top_p_regen\"],\n",
    "        repetition_penalty=CONFIG[\"repetition_penalty_regen\"],\n",
    "    ),\n",
    ")\n",
    "print(\"Text->TikZ model ready on\", text_to_tikz_runner.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c49e61a",
   "metadata": {},
   "source": [
    "## 7. Regenerate TikZ Code from Description\n",
    "Feed the generated instructions into AutomaTikZ, buffering partial output so retries can continue even if GPU memory pressure forces the model onto CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b1a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: convert description back into TikZ\n",
    "if not RUN_STATE.get(\"description\"):\n",
    "    raise ValueError(\"Description text missing. Rerun Section 5 before regenerating TikZ.\")\n",
    "\n",
    "try:\n",
    "    regenerated_tikz = text_to_tikz_runner.generate(\n",
    "        RUN_STATE[\"description\"],\n",
    "        overrides={\n",
    "            \"max_new_tokens\": CONFIG[\"max_tokens_regen\"],\n",
    "            \"temperature\": CONFIG[\"temperature_regen\"],\n",
    "            \"top_p\": CONFIG[\"top_p_regen\"],\n",
    "            \"repetition_penalty\": CONFIG[\"repetition_penalty_regen\"],\n",
    "        },\n",
    "    )\n",
    "    RUN_STATE[\"regenerated_tikz\"] = regenerated_tikz\n",
    "    print(f\"Regenerated TikZ length: {len(regenerated_tikz.split())} tokens\")\n",
    "except Exception as err:\n",
    "    logging.exception(\"Text-to-TikZ generation failed: %s\", err)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46babe8",
   "metadata": {},
   "source": [
    "## 8. Optionally Compile TikZ to PDF with Robust Logging\n",
    "Invoke `pdflatex` only when requested, capturing output and skipping gracefully if TeX tooling is unavailable on the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b92f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8: optional PDF compilation\n",
    "pdf_output_path = None\n",
    "if CONFIG[\"compile_pdf\"]:\n",
    "    if \"regenerated_tikz\" not in RUN_STATE:\n",
    "        raise ValueError(\"No regenerated TikZ found. Run Section 7 before compiling.\")\n",
    "    tex_document = \"\"\"\\\\documentclass[tikz,border=5pt]{standalone}\n",
    "\\\\usepackage{tikz}\n",
    "\\\\usepackage{pgfplots}\n",
    "\\\\pgfplotsset{compat=1.18}\n",
    "\\\\begin{document}\n",
    "%s\n",
    "\\\\end{document}\n",
    "\"\"\" % RUN_STATE[\"regenerated_tikz\"]\n",
    "    tex_path = ARTIFACTS_DIR / f\"{CONFIG['job_name']}.tex\"\n",
    "    tex_path.write_text(tex_document, encoding=\"utf-8\")\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\n",
    "                \"pdflatex\",\n",
    "                \"-interaction=nonstopmode\",\n",
    "                \"-halt-on-error\",\n",
    "                f\"-output-directory={ARTIFACTS_DIR.as_posix()}\",\n",
    "                tex_path.as_posix(),\n",
    "            ],\n",
    "            check=True,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "        )\n",
    "        print(result.stdout)\n",
    "        pdf_output_path = ARTIFACTS_DIR / f\"{CONFIG['job_name']}.pdf\"\n",
    "        if pdf_output_path.exists():\n",
    "            RUN_STATE[\"compiled_pdf\"] = str(pdf_output_path)\n",
    "            print(f\"PDF generated at {pdf_output_path}\")\n",
    "    except FileNotFoundError:\n",
    "        logging.warning(\"pdflatex not installed. Skipping PDF compilation.\")\n",
    "    except subprocess.CalledProcessError as err:\n",
    "        logging.error(\"pdflatex failed: %s\", err.stdout)\n",
    "else:\n",
    "    print(\"PDF compilation skipped (set CONFIG['compile_pdf'] = True to enable).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db781d14",
   "metadata": {},
   "source": [
    "## 9. Persist Outputs and Summarise Artifacts\n",
    "Write intermediate files (TikZ copy, description, regenerated TikZ, optional PDF) and a JSON manifest so downstream scripts can locate artifacts easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca96e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 9: persist artefacts and emit summary\n",
    "summary = {\n",
    "    \"tikz_source_path\": str(CONFIG[\"tikz_path\"]),\n",
    "    \"artifacts_dir\": str(ARTIFACTS_DIR),\n",
    "    \"tikz_to_text_model\": CONFIG[\"tikz_to_text_model\"],\n",
    "    \"text_to_tikz_model\": CONFIG[\"text_to_tikz_model\"],\n",
    "    \"description_tokens\": len(RUN_STATE.get(\"description\", \"\").split()),\n",
    "}\n",
    "\n",
    "extracted_path = ARTIFACTS_DIR / \"extracted_tikz.tex\"\n",
    "extracted_path.write_text(RUN_STATE.get(\"tikz_source\", \"\"), encoding=\"utf-8\")\n",
    "summary[\"extracted_tikz\"] = str(extracted_path)\n",
    "\n",
    "description_path = ARTIFACTS_DIR / \"generated_description.txt\"\n",
    "description_path.write_text(RUN_STATE.get(\"description\", \"\"), encoding=\"utf-8\")\n",
    "summary[\"description_file\"] = str(description_path)\n",
    "\n",
    "if \"regenerated_tikz\" in RUN_STATE:\n",
    "    regen_path = ARTIFACTS_DIR / \"regenerated_tikz.tex\"\n",
    "    regen_path.write_text(RUN_STATE[\"regenerated_tikz\"], encoding=\"utf-8\")\n",
    "    summary[\"regenerated_tikz_file\"] = str(regen_path)\n",
    "\n",
    "if \"compiled_pdf\" in RUN_STATE:\n",
    "    summary[\"compiled_pdf\"] = RUN_STATE[\"compiled_pdf\"]\n",
    "\n",
    "summary_path = ARTIFACTS_DIR / \"pipeline_summary.json\"\n",
    "summary_path.write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n",
    "print(\"Summary written to\", summary_path)\n",
    "print(json.dumps(summary, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
