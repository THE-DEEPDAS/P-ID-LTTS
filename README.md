# P&ID Digitalisation Toolkit

This repository contains two complementary workflows:

1. **Supervised fine-tuning (`finetune.py`)** – train Llama 3 7B to convert TikZ-based P&ID diagrams into rich natural-language instructions.
2. **End-to-end digitalisation (`pipeline.py`)** – automate TikZ→natural-language→TikZ regeneration (with optional PDF compilation), optimised for Kaggle notebooks.

## Dataset Format for Fine-Tuning

Your dataset should be placed in a folder named `data` in the root of your project. The recommended format is a single JSONL (JSON Lines) file, where each line is a JSON object with two fields:

- `prompt`: The LaTeX (TikZ) code describing the P&ID diagram.
- `response`: The detailed natural language instructions explaining all relations, components, and how to remake the diagram.

### Example (`data/train.jsonl`):
```jsonl
{"prompt": "\\begin{tikzpicture} ... \\end{tikzpicture}", "response": "Draw a pump connected to a valve..."}
{"prompt": "\\begin{tikzpicture} ... \\end{tikzpicture}", "response": "Start by drawing a tank..."}
```

- Each line is a separate training example.
- You may also create a `validation.jsonl` file for validation during training.

## Folder Structure
```
project-root/
├── data/
│   ├── train.jsonl
│   └── validation.jsonl
├── finetune.py
└── ...
```

## Notes
- Ensure TikZ code is valid LaTeX and the response is a clear, step-by-step instruction.
- More examples will generally improve model performance.
- For best results, keep the prompt and response fields concise but complete.

## Kaggle Quickstart for `pipeline.py`

### 1. Enable GPU and install dependencies

Run the following in a Kaggle notebook cell before executing the pipeline:

```bash
pip install transformers==4.44.0 accelerate==0.33.0 bitsandbytes==0.43.1
pip install git+https://github.com/potamides/AutomaTikZ
```

If you plan to compile regenerated TikZ into PDFs, add TeX Live (takes a few minutes and ~4 GB disk space):

```bash
sudo apt-get update
sudo apt-get install -y texlive-full
```

### 2. Provide a Hugging Face token (if models are gated)

```bash
export HF_TOKEN="hf_..."
```

Alternatively, set `USER_PIPELINE_OVERRIDES["hf_token"]` inside `pipeline.py` or rely on the
environment variable at runtime. Avoid hard-coding organisation tokens inside notebooks. The default
analysis model (`meta-llama/Llama-3.1-8B-Instruct`) is a gated repository—request access on its
model card and supply a valid token before running the pipeline.

### 3. Point the pipeline to your TikZ file

Place the LaTeX/TikZ source in your notebook workspace (for example the Kaggle input dataset), then
edit the `USER_PIPELINE_OVERRIDES` section near the top of `pipeline.py`:

```python
USER_PIPELINE_OVERRIDES = {
    "tex_path": "/kaggle/input/tex-tikz/main.tex",
    "output_dir": "artifacts",
    "compile": True,
    "verbose": True,
}
```

Once the overrides are saved, simply execute the script:

```bash
python pipeline.py
```

The pipeline merges overrides with baked-in defaults, so you can focus on editing a small dictionary
instead of passing CLI flags. This is especially convenient when you launch the script via the Kaggle
“Run” button, which does not expose argument prompts.

> If you do not have access to the default Llama 3 model, either request it from Meta/Hugging Face or
> override `tikz_to_text_model`/`text_to_tikz_model` with public checkpoints you can download.

### Outputs

The script creates a structured `artifacts/` directory containing:

- `extracted_tikz.tex` – TikZ copied from the provided source (kept for traceability)
- `generated_description.txt` – detailed instructions from Llama 3 8B
- `regenerated_tikz.tex` – refined TikZ generated by AutomaTikZ
- `pid_digitalised.pdf` – optional compiled PDF (when `--compile` is used)
- `pipeline_summary.json` – quick pointers to all artefacts and model metadata

### Configuring the pipeline

The following keys are available inside `DEFAULT_PIPELINE_SETTINGS` or `USER_PIPELINE_OVERRIDES`:

- `tex_path` – path to the TikZ source (defaults to `/kaggle/input/tex-tikz/main.tex`).
- `output_dir` – where intermediate and final artefacts are written.
- `tikz_to_text_model` – Hugging Face repo for the analysis model (default: `meta-llama/Llama-3.1-8B-Instruct`).
- `text_to_tikz_model` – repo for the generator model (default: `potamides/AutomaTikZ`).
- `hf_token` – optional Hugging Face access token (falls back to the `HF_TOKEN` environment variable).
- `compile` – set to `True` to generate PDFs via `pdflatex`.
- `keep_intermediates` – keep `.aux/.log/.out` files produced by TeX.
- `full_precision` – load models in full precision instead of 4-bit quantisation.
- `job_name` – stem used for TeX/PDF artefacts during compilation.
- `verbose` – enable debug-level logging for deeper inspection.

## Pipeline Overview

1. **TikZ ingestion** – Loads the provided LaTeX/TikZ file and stores a copy for traceability.
2. **TikZ → Natural language** – Llama 3 8B produces exhaustive, plant-engineer-friendly instructions covering assets, flows, and layout.
3. **Natural language → TikZ** – AutomaTikZ regenerates structured TikZ aligned with the textual brief.
4. **Optional compilation** – Wraps the TikZ in a minimal document class and invokes `pdflatex` for quick visual inspection.

You can run each step independently by reusing the classes in `pipeline.py`, making it easy to debug intermediate artefacts or insert custom QA checks.
